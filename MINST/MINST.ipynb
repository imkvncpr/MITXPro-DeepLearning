{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sizes = {'train': 50000, 'val': 10000, 'test': 10000}\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.4704 Acc: 0.8634\n",
      "val Loss: 0.2661 Acc: 0.9208\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2344 Acc: 0.9302\n",
      "val Loss: 0.1898 Acc: 0.9428\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.1692 Acc: 0.9497\n",
      "val Loss: 0.1614 Acc: 0.9530\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1327 Acc: 0.9602\n",
      "val Loss: 0.1420 Acc: 0.9571\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1103 Acc: 0.9663\n",
      "val Loss: 0.1363 Acc: 0.9574\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.0939 Acc: 0.9709\n",
      "val Loss: 0.1191 Acc: 0.9628\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9743\n",
      "val Loss: 0.1215 Acc: 0.9620\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.0547 Acc: 0.9835\n",
      "val Loss: 0.0860 Acc: 0.9754\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0496 Acc: 0.9854\n",
      "val Loss: 0.0849 Acc: 0.9756\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0475 Acc: 0.9862\n",
      "val Loss: 0.0842 Acc: 0.9756\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0457 Acc: 0.9868\n",
      "val Loss: 0.0838 Acc: 0.9760\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0441 Acc: 0.9874\n",
      "val Loss: 0.0834 Acc: 0.9760\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0425 Acc: 0.9880\n",
      "val Loss: 0.0829 Acc: 0.9763\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0411 Acc: 0.9886\n",
      "val Loss: 0.0827 Acc: 0.9764\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0384 Acc: 0.9899\n",
      "val Loss: 0.0797 Acc: 0.9762\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.9899\n",
      "val Loss: 0.0796 Acc: 0.9761\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0372 Acc: 0.9902\n",
      "val Loss: 0.0795 Acc: 0.9756\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0369 Acc: 0.9902\n",
      "val Loss: 0.0794 Acc: 0.9758\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0368 Acc: 0.9903\n",
      "val Loss: 0.0794 Acc: 0.9759\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0366 Acc: 0.9903\n",
      "val Loss: 0.0793 Acc: 0.9760\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0364 Acc: 0.9903\n",
      "val Loss: 0.0793 Acc: 0.9761\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0359 Acc: 0.9905\n",
      "val Loss: 0.0793 Acc: 0.9763\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0359 Acc: 0.9905\n",
      "val Loss: 0.0793 Acc: 0.9764\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0359 Acc: 0.9904\n",
      "val Loss: 0.0794 Acc: 0.9764\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0358 Acc: 0.9905\n",
      "val Loss: 0.0794 Acc: 0.9764\n",
      "Best val Acc: 0.976400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time, copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics \n",
    "\n",
    "\n",
    "# These transforms will be performed on every datapoint - in this example we want to transform every\n",
    "# datapoint to a Tensor datatype, and perform\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "mnist_train = torchvision.datasets.MNIST('', train=True, transform =transform, download=True)\n",
    "# We will split out train dataset into train and validation!\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
    "mnist_test = torchvision.datasets.MNIST('', train=False, transform = transform, download=True)\n",
    "\n",
    "# We will create DataLoaders just like before with a batch size of 100\n",
    "batch_size = 100\n",
    "dataloaders = {'train': DataLoader(mnist_train, batch_size=batch_size),\n",
    "               'val': DataLoader(mnist_val, batch_size=batch_size),\n",
    "               'test': DataLoader(mnist_test, batch_size=batch_size)}\n",
    "\n",
    "dataset_sizes = {'train': len(mnist_train),\n",
    "                 'val': len(mnist_val),\n",
    "                 'test': len(mnist_test)}\n",
    "print(f'dataset_sizes = {dataset_sizes}')\n",
    "\n",
    "# Hint! In the Module 3 Introduction to Pytorch notebook, the Network\n",
    "# we created required the input data to be of shape Nx1 where N is the number of\n",
    "# features. Currently, our MNIST dataset is shape 28x28 as they are images. Use \n",
    "# this code snippet as you iterate through the datapoint in your dataset to flatten\n",
    "# them so it is size 784x1 and can be used with the models we designed previously!\n",
    "\n",
    "# This loop only iterates through the \"train\" datapoints\n",
    "# In the previous notebook\n",
    "phases = [\"train\", \"val\", \"test\"]\n",
    "for phase in phases:\n",
    "  for inputs, labels in dataloaders[phase]:\n",
    "    # This flattens every every batch to the correct size!\n",
    "    inputs = inputs.view(inputs.shape[0],-1)\n",
    "    \n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    training_curves = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            training_curves[f'{phase}_loss'].append(epoch_loss)\n",
    "            training_curves[f'{phase}_acc'].append(epoch_acc)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, training_curves\n",
    "\n",
    "# Train the model and save the training curves\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model, training_curves = train_model(model, criterion, optimizer, scheduler, num_epochs=25)\n",
    "\n",
    "# Utility functions for plotting your results!\n",
    "def plot_training_curves(training_curves, \n",
    "                         phases=['train', 'val', 'test'],\n",
    "                         metrics=['loss','acc']):\n",
    "    epochs = list(range(len(training_curves['train_loss'])))\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.title(f'Training curves - {metric}')\n",
    "        for phase in phases:\n",
    "            key = phase+'_'+metric\n",
    "            if key in training_curves:\n",
    "                plt.plot(epochs, training_curves[phase+'_'+metric])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(labels=phases)\n",
    "\n",
    "def classify_predictions(model, device, dataloader):\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    all_labels = torch.tensor([]).to(device)\n",
    "    all_scores = torch.tensor([]).to(device)\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    for inputs, labels in dataloader:\n",
    "        # Important! We need to flatten every datapoint\n",
    "        inputs = inputs.view(inputs.shape[0], -1)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = torch.softmax(model(inputs),dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        scores = outputs[:,1]\n",
    "        all_labels = torch.cat((all_labels, labels), 0)\n",
    "        all_scores = torch.cat((all_scores, scores), 0)\n",
    "        all_preds = torch.cat((all_preds, preds), 0)\n",
    "    return all_preds.detach().cpu(), all_labels.detach().cpu(), all_scores.detach().cpu()\n",
    "\n",
    "def plot_cm(model, device, dataloaders, phase='test'):\n",
    "    class_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    preds, labels, scores = classify_predictions(model, device, dataloaders[phase])\n",
    "    \n",
    "    cm = metrics.confusion_matrix(labels, preds)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    ax = disp.plot().ax_\n",
    "    ax.set_title('Confusion Matrix -- counts')\n",
    "    \n",
    "    plot_training_curves(training_curves, phases=['train', 'val', 'test'])\n",
    "    \n",
    "    res = plot_cm(model, device, dataloaders, phase='test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion_Minst.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
