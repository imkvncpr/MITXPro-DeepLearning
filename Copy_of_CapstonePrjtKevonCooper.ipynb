{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLh2FTlTsXcHMTebMNqDd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imkvncpr/MITXPro-DeepLearning/blob/main/Copy_of_CapstonePrjtKevonCooper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "Model = tf.keras.models.Model\n",
        "Sequential = tf.keras.models.Sequential\n",
        "Dense = tf.keras.layers.Dense\n",
        "LSTM = tf.keras.layers.LSTM\n",
        "Concatenate = tf.keras.layers.Concatenate\n",
        "Input = tf.keras.layers.Input\n",
        "Dropout = tf.keras.layers.Dropout\n",
        "EarlyStopping = tf.keras.callbacks.EarlyStopping\n",
        "ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint\n",
        "import plotly.graph_objects as go\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional, Union\n",
        "import os\n",
        "\n",
        "# Step 1: Set up logging configuration for debugging and monitoring\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CryptoPricePredictor:\n",
        "    def __init__(self,\n",
        "                 ticker: str = 'BTC-USD',\n",
        "                 look_back: int = 60,\n",
        "                 test_size: float = 0.2,\n",
        "                 random_state: int = 42):\n",
        "        \"\"\"\n",
        "        Step 2: Initialize the predictor with configuration parameters\n",
        "        - ticker: The cryptocurrency symbol to predict (e.g., BTC-USD)\n",
        "        - look_back: Number of past days to consider for prediction\n",
        "        - test_size: Portion of data reserved for testing\n",
        "        - random_state: Seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.ticker = ticker\n",
        "        self.look_back = look_back\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))  # Scale data between 0 and 1\n",
        "        self.model = None\n",
        "        self.X = None\n",
        "        self.last_sequence = None  # Used for future predictions\n",
        "\n",
        "    def download_crypto_data(self, years: int = 5) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Step 3: Download historical price data using yfinance\n",
        "        - Fetches daily OHLCV data for specified number of years\n",
        "        - Returns None if download fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            end_date = pd.Timestamp.now()\n",
        "            start_date = end_date - pd.DateOffset(years=years)\n",
        "            data = yf.download(self.ticker, start=start_date, end=end_date)\n",
        "            logger.info(f\"Successfully downloaded {years} years of {self.ticker} data\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error downloading data: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def prepare_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Step 4: Prepare data for LSTM model\n",
        "        - Creates sequences of look_back days for training\n",
        "        - Scales all features using MinMaxScaler\n",
        "        - Returns X (features) and y (target) arrays\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Extract relevant features from data\n",
        "            feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "            data_processed = data[feature_columns].values\n",
        "\n",
        "            # Scale features to range [0,1]\n",
        "            scaled_data = self.scaler.fit_transform(data_processed)\n",
        "\n",
        "            # Create sequences for LSTM\n",
        "            X, y = [], []\n",
        "            for i in range(self.look_back, len(scaled_data)):\n",
        "                X.append(scaled_data[i-self.look_back:i])  # Sequence of look_back days\n",
        "                y.append(scaled_data[i, 3])  # Target is next day's closing price\n",
        "\n",
        "            X, y = np.array(X), np.array(y)\n",
        "            self.last_sequence = X[-1:]  # Save last sequence for future predictions\n",
        "            logger.info(f\"Prepared data shapes: X: {X.shape}, y: {y.shape}\")\n",
        "            return X, y\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_model(self,\n",
        "                    neurons: int = 100,\n",
        "                    dropout: float = 0.3) -> tf.keras.models.Sequential:\n",
        "        \"\"\"\n",
        "        Step 5: Create LSTM neural network architecture\n",
        "        - Two LSTM layers with decreasing units\n",
        "        - Dropout layers for regularization\n",
        "        - Dense layers for final prediction\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model = Sequential([\n",
        "                # First LSTM layer with return sequences for stacking\n",
        "                LSTM(neurons, return_sequences=True,\n",
        "                     input_shape=(self.look_back, 5)),\n",
        "                Dropout(dropout),  # Prevent overfitting\n",
        "\n",
        "                # Second LSTM layer without return sequences\n",
        "                LSTM(neurons//2, return_sequences=False),\n",
        "                Dropout(dropout/2),\n",
        "\n",
        "                # Dense layers for final prediction\n",
        "                Dense(25, activation='relu'),\n",
        "                Dense(1)  # Output layer for price prediction\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def train_model(self,\n",
        "                   X: np.ndarray,\n",
        "                   y: np.ndarray,\n",
        "                   optimize_hyperparameters: bool = False,\n",
        "                   **kwargs) -> Tuple[tf.keras.models.Sequential, Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Step 6: Train the LSTM model\n",
        "        - Splits data into train/test sets\n",
        "        - Optional hyperparameter optimization\n",
        "        - Uses early stopping to prevent overfitting\n",
        "        - Returns trained model and performance metrics\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Split data into training and testing sets\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=self.test_size, random_state=self.random_state\n",
        "            )\n",
        "\n",
        "            if optimize_hyperparameters:\n",
        "                model = self._optimize_hyperparameters(X_train, y_train)\n",
        "            else:\n",
        "                model = self.create_model()\n",
        "\n",
        "                # Set up callbacks for training\n",
        "                callbacks = [\n",
        "                    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "                    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "                ]\n",
        "\n",
        "                # Train the model\n",
        "                model.fit(\n",
        "                    X_train, y_train,\n",
        "                    epochs=kwargs.get('epochs', 100),\n",
        "                    batch_size=kwargs.get('batch_size', 32),\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "            # Evaluate model performance\n",
        "            predictions = model.predict(X_test, verbose=0)\n",
        "            metrics = self._calculate_metrics(y_test, predictions)\n",
        "\n",
        "            self.model = model\n",
        "            return model, metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict_future(self, days: int = 30) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Step 7: Generate future price predictions\n",
        "        - Uses the last known sequence to predict next day\n",
        "        - Updates sequence with prediction for next prediction\n",
        "        - Returns array of predicted prices in original scale\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model must be trained before making predictions\")\n",
        "\n",
        "        if self.last_sequence is None:\n",
        "            raise ValueError(\"No data sequence available for prediction\")\n",
        "\n",
        "        try:\n",
        "            current_sequence = np.copy(self.last_sequence)\n",
        "            future_predictions = []\n",
        "\n",
        "            # Predict prices for specified number of days\n",
        "            for _ in range(days):\n",
        "                # Predict next day's price\n",
        "                next_pred = self.model.predict(current_sequence, verbose=0)[0]\n",
        "                future_predictions.append(next_pred)\n",
        "\n",
        "                # Update sequence for next prediction\n",
        "                new_sequence = np.copy(current_sequence)\n",
        "                new_sequence[0, :-1] = new_sequence[0, 1:]\n",
        "                new_sequence[0, -1] = new_sequence[0, -2]\n",
        "                new_sequence[0, -1, 3] = next_pred\n",
        "\n",
        "                current_sequence = new_sequence\n",
        "\n",
        "            # Convert predictions back to original price scale\n",
        "            future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
        "            future_predictions_reshaped = np.hstack([\n",
        "                np.zeros((len(future_predictions), 3)),\n",
        "                future_predictions,\n",
        "                np.zeros((len(future_predictions), 1))\n",
        "            ])\n",
        "\n",
        "            future_prices = self.scaler.inverse_transform(future_predictions_reshaped)[:, 3]\n",
        "\n",
        "            logger.info(f\"Generated {days} days of future predictions\")\n",
        "            return future_prices\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error making future predictions: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _optimize_hyperparameters(self, X_train: np.ndarray, y_train: np.ndarray) -> tf.keras.models.Sequential:\n",
        "        \"\"\"\n",
        "        Step 8: Optimize model hyperparameters\n",
        "        - Uses RandomizedSearchCV for parameter tuning\n",
        "        - Tests different combinations of neurons, dropout, batch size, and epochs\n",
        "        \"\"\"\n",
        "        model = KerasRegressor(model=self.create_model, verbose=0)\n",
        "\n",
        "        param_dist = {\n",
        "            'model__neurons': [50, 100, 150, 200],\n",
        "            'model__dropout': [0.2, 0.3, 0.4, 0.5],\n",
        "            'batch_size': [16, 32, 64],\n",
        "            'epochs': [50, 100, 150]\n",
        "        }\n",
        "\n",
        "        random_search = RandomizedSearchCV(\n",
        "            estimator=model,\n",
        "            param_distributions=param_dist,\n",
        "            n_iter=10,\n",
        "            cv=3,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        random_search.fit(X_train, y_train)\n",
        "        logger.info(f\"Best hyperparameters: {random_search.best_params_}\")\n",
        "        return random_search.best_estimator_.model_\n",
        "\n",
        "    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Step 9: Calculate performance metrics\n",
        "        - Converts predictions back to original scale\n",
        "        - Calculates MSE, RMSE, MAE, and R2 score\n",
        "        \"\"\"\n",
        "        # Convert back to original price scale\n",
        "        y_true_orig = self.scaler.inverse_transform(np.column_stack([np.zeros((len(y_true), 3)),\n",
        "                                                                    y_true.reshape(-1, 1),\n",
        "                                                                    np.zeros((len(y_true), 1))]))[:, 3]\n",
        "        y_pred_orig = self.scaler.inverse_transform(np.column_stack([np.zeros((len(y_pred), 3)),\n",
        "                                                                    y_pred.reshape(-1, 1),\n",
        "                                                                    np.zeros((len(y_pred), 1))]))[:, 3]\n",
        "\n",
        "        return {\n",
        "            'MSE': mean_squared_error(y_true_orig, y_pred_orig),\n",
        "            'RMSE': np.sqrt(mean_squared_error(y_true_orig, y_pred_orig)),\n",
        "            'MAE': mean_absolute_error(y_true_orig, y_pred_orig),\n",
        "            'R2': r2_score(y_true_orig, y_pred_orig)\n",
        "        }\n",
        "\n",
        "    def visualize_predictions(self, y_true: np.ndarray, y_pred: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Step 10: Visualize predictions vs actual values\n",
        "        - Creates interactive plot using plotly\n",
        "        - Shows actual and predicted prices on same graph\n",
        "        \"\"\"\n",
        "        # Convert predictions to original scale\n",
        "        y_true_orig = self.scaler.inverse_transform(np.column_stack([np.zeros((len(y_true), 3)),\n",
        "                                                                    y_true.reshape(-1, 1),\n",
        "                                                                    np.zeros((len(y_true), 1))]))[:, 3]\n",
        "        y_pred_orig = self.scaler.inverse_transform(np.column_stack([np.zeros((len(y_pred), 3)),\n",
        "                                                                    y_pred.reshape(-1, 1),\n",
        "                                                                    np.zeros((len(y_pred), 1))]))[:, 3]\n",
        "\n",
        "        # Create interactive plot\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(y=y_true_orig, mode='lines', name='Actual Prices'))\n",
        "        fig.add_trace(go.Scatter(y=y_pred_orig, mode='lines', name='Predicted Prices'))\n",
        "        fig.update_layout(\n",
        "            title=f'{self.ticker} Price Prediction',\n",
        "            xaxis_title='Time',\n",
        "            yaxis_title='Price',\n",
        "            template='plotly_dark'\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Step 11: Main execution function\n",
        "    - Demonstrates complete prediction pipeline\n",
        "    - Handles errors and displays results\n",
        "    \"\"\"\n",
        "    # Initialize predictor with default settings\n",
        "    predictor = CryptoPricePredictor(ticker='BTC-USD', look_back=60)\n",
        "\n",
        "    try:\n",
        "        # Execute prediction pipeline\n",
        "        logger.info(\"Downloading cryptocurrency data...\")\n",
        "        data = predictor.download_crypto_data(years=5)\n",
        "        if data is None:\n",
        "            logger.error(\"Failed to download data\")\n",
        "            return\n",
        "\n",
        "        logger.info(\"Preparing data for training...\")\n",
        "        X, y = predictor.prepare_data(data)\n",
        "        predictor.X = X\n",
        "\n",
        "        logger.info(\"Training the model...\")\n",
        "        model, metrics = predictor.train_model(\n",
        "            X, y,\n",
        "            optimize_hyperparameters=False,\n",
        "            epochs=100,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\nModel Performance Metrics:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "        logger.info(\"Generating historical predictions visualization...\")\n",
        "        y_pred = model.predict(X, verbose=0)\n",
        "        predictor.visualize_predictions(y, y_pred)\n",
        "\n",
        "        logger.info(\"Generating future price predictions...\")\n",
        "        future_prices = predictor.predict_future(days=30)\n",
        "        print(\"\\nPredicted prices for next 30 days:\")\n",
        "        for i, price in enumerate(future_prices, 1):\n",
        "            print(f\"Day {i}: ${price:,.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred in main: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8c-mpXCOMik8",
        "outputId": "676b3527-e3a2-40a2-d2d4-16d2d748ad66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning:\n",
            "\n",
            "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0311"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 94ms/step - loss: 0.0307 - val_loss: 0.0037\n",
            "Epoch 2/100\n",
            "\u001b[1m35/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0036"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 0.0036 - val_loss: 0.0015\n",
            "Epoch 3/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0029"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - loss: 0.0028 - val_loss: 0.0013\n",
            "Epoch 4/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0022"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.0022 - val_loss: 0.0012\n",
            "Epoch 5/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0021"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 0.0021 - val_loss: 0.0011\n",
            "Epoch 6/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - loss: 0.0019 - val_loss: 0.0013\n",
            "Epoch 7/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0022"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0022 - val_loss: 9.8457e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 0.0019 - val_loss: 0.0012\n",
            "Epoch 9/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - loss: 0.0019 - val_loss: 0.0013\n",
            "Epoch 10/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0014"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.0014 - val_loss: 9.7755e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m35/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.0018"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0018 - val_loss: 9.5108e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.0016"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.0016 - val_loss: 9.0300e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 14/100\n",
            "\u001b[1m35/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0015"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - loss: 0.0015 - val_loss: 7.5578e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - loss: 0.0013 - val_loss: 8.4593e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 17/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0014"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 0.0014 - val_loss: 7.0507e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 20/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0018 - val_loss: 8.1450e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - loss: 0.0011 - val_loss: 8.1376e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0011"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - loss: 0.0011 - val_loss: 6.2746e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - loss: 0.0013 - val_loss: 6.4276e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 0.0011 - val_loss: 8.5954e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 26/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 115ms/step - loss: 0.0013 - val_loss: 6.7429e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 0.0011 - val_loss: 8.2198e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0012 - val_loss: 6.3133e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.0011"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 130ms/step - loss: 0.0011 - val_loss: 5.8931e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 9.6947e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 9.7029e-04 - val_loss: 5.4927e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 9.8850e-04 - val_loss: 8.9204e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 136ms/step - loss: 9.2269e-04 - val_loss: 5.9176e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 9.2015e-04 - val_loss: 9.3886e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0012 - val_loss: 5.9663e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 9.3713e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 9.3744e-04 - val_loss: 5.2408e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - loss: 9.2919e-04 - val_loss: 5.5869e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 8.4592e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 8.4746e-04 - val_loss: 4.9765e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - loss: 9.8128e-04 - val_loss: 6.1452e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - loss: 8.6088e-04 - val_loss: 5.1413e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 8.0602e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 8.0666e-04 - val_loss: 4.8813e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 8.8359e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 8.8423e-04 - val_loss: 4.7029e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 130ms/step - loss: 7.2544e-04 - val_loss: 5.8338e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 9.5363e-04 - val_loss: 5.4593e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 7.1957e-04 - val_loss: 5.7644e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 7.6861e-04 - val_loss: 4.8947e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - loss: 9.0460e-04 - val_loss: 4.8867e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 7.0985e-04 - val_loss: 5.6991e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 6.3767e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 6.3810e-04 - val_loss: 4.3377e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 110ms/step - loss: 9.0761e-04 - val_loss: 5.2641e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 7.8992e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 7.8990e-04 - val_loss: 4.2766e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 7.7220e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 7.7086e-04 - val_loss: 4.2354e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - loss: 7.9605e-04 - val_loss: 5.6567e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 7.9512e-04 - val_loss: 5.0515e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - loss: 7.3454e-04 - val_loss: 4.3132e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - loss: 7.5798e-04 - val_loss: 4.7035e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 7.3282e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 7.3118e-04 - val_loss: 3.8914e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - loss: 6.8957e-04 - val_loss: 5.2386e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 6.4414e-04 - val_loss: 8.2922e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 6.7441e-04 - val_loss: 4.3641e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 5.7213e-04 - val_loss: 4.8085e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 7.1413e-04 - val_loss: 6.4896e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 9.0914e-04 - val_loss: 4.0695e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 7.5781e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 7.5754e-04 - val_loss: 3.6198e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - loss: 7.3958e-04 - val_loss: 3.6273e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 79ms/step - loss: 6.8432e-04 - val_loss: 4.3013e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 7.7458e-04 - val_loss: 3.7999e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - loss: 7.2118e-04 - val_loss: 4.6978e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - loss: 7.1485e-04 - val_loss: 6.1567e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 8.2033e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - loss: 8.1905e-04 - val_loss: 3.5959e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - loss: 6.7297e-04 - val_loss: 4.5836e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - loss: 5.7279e-04 - val_loss: 3.6028e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 5.5973e-04 - val_loss: 3.9311e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 6.0777e-04 - val_loss: 3.8951e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - loss: 5.7916e-04 - val_loss: 7.1053e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - loss: 7.1005e-04 - val_loss: 3.8919e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 5.9888e-04 - val_loss: 3.7795e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 5.7729e-04 - val_loss: 3.9627e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 4.8979e-04 - val_loss: 4.1048e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 5.9069e-04 - val_loss: 4.6180e-04\n",
            "\n",
            "Model Performance Metrics:\n",
            "MSE: 3256525.76\n",
            "RMSE: 1804.58\n",
            "MAE: 1359.08\n",
            "R2: 0.99\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"26412f81-f463-4203-ba9b-edf54c270cd8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"26412f81-f463-4203-ba9b-edf54c270cd8\")) {                    Plotly.newPlot(                        \"26412f81-f463-4203-ba9b-edf54c270cd8\",                        [{\"mode\":\"lines\",\"name\":\"Actual Prices\",\"y\":[6865.493164062501,6859.0830078125,6971.091796875,6845.03759765625,6842.427734375,6642.10986328125,7116.80419921875,7096.1845703125,7257.6650390625,7189.4248046875,6881.95849609375,6880.323242187499,7117.20751953125,7429.724609375,7550.90087890625,7569.93603515625,7679.8671875,7795.60107421875,7807.05859375,8801.0380859375,8658.5537109375,8864.7666015625,8988.5966796875,8897.46875,8912.654296875,9003.0703125,9268.76171875,9951.5185546875,9842.666015625,9593.896484375,8756.4306640625,8601.7958984375,8804.4775390625,9269.9873046875,9733.7216796875,9328.197265625,9377.013671875,9670.7392578125,9726.5751953125,9729.0380859375,9522.9814453125,9081.76171875,9182.5771484375,9209.287109375,8790.3681640625,8906.9345703125,8835.052734375,9181.017578125,9525.7509765625,9439.1240234375,9700.4140625,9461.05859375,10167.2685546875,9529.8037109375,9656.7177734375,9800.63671875,9665.533203125,9653.6796875,9758.8525390625,9771.4892578125,9795.7001953125,9870.0947265625,9321.78125,9480.84375,9475.27734375,9386.7880859375,9450.7021484375,9538.0244140625,9480.2548828125,9411.8408203125,9288.0185546875,9332.3408203125,9303.6298828125,9648.7177734375,9629.658203125,9313.6103515625,9264.8134765625,9162.91796875,9045.390625,9143.58203125,9190.8544921875,9137.9931640625,9228.3251953125,9123.41015625,9087.3037109375,9132.48828125,9073.9423828125,9375.474609375,9252.27734375,9428.3330078125,9277.9677734375,9278.8076171875,9240.3466796875,9276.5,9243.6142578125,9243.2138671875,9192.8369140625,9132.2275390625,9151.392578125,9159.0400390625,9185.8173828125,9164.2314453125,9374.8876953125,9525.36328125,9581.072265625,9536.892578125,9677.11328125,9905.1669921875,10990.873046875,10912.8232421875,11100.4677734375,11111.2138671875,11323.466796875,11759.5927734375,11053.6142578125,11246.3486328125,11205.892578125,11747.0224609375,11779.7734375,11601.47265625,11754.0458984375,11675.7392578125,11878.111328125,11410.525390625,11584.9345703125,11784.1376953125,11768.87109375,11865.6982421875,11892.8037109375,12254.40234375,11991.2333984375,11758.283203125,11878.3720703125,11592.4892578125,11681.8251953125,11664.84765625,11774.595703125,11366.134765625,11488.36328125,11323.3974609375,11542.5,11506.865234375,11711.505859375,11680.8203125,11970.478515625,11414.0341796875,10245.296875,10511.8134765625,10169.5673828125,10280.3515625,10369.5634765625,10131.5166015625,10242.34765625,10363.138671875,10400.9150390625,10442.1708984375,10323.755859375,10680.837890625,10796.951171875,10974.9052734375,10948.990234375,10944.5859375,11094.3466796875,10938.271484375,10462.259765625,10538.4599609375,10246.1865234375,10760.06640625,10692.716796875,10750.7236328125,10775.26953125,10709.65234375,10844.640625,10784.4912109375,10619.4521484375,10575.974609375,10549.3291015625,10669.5830078125,10793.33984375,10604.40625,10668.96875,10915.685546875,11064.4580078125,11296.361328125,11384.181640625,11555.36328125,11425.8994140625,11429.5068359375,11495.349609375,11322.123046875,11358.1015625,11483.359375,11742.037109375,11916.3349609375,12823.689453125,12965.891601562502,12931.5390625,13108.062499999998,13031.173828125,13075.248046874998,13654.21875,13271.28515625,13437.8828125,13546.5224609375,13780.9951171875,13737.109375,13550.4892578125,13950.30078125,14133.70703125,15579.8486328125,15565.880859375002,14833.753906249998,15479.567382812502,15332.3154296875,15290.90234375,15701.33984375,16276.34375,16317.80859375,16068.138671875,15955.587890625,16716.111328125,17645.40625,17804.005859375,17817.089843749996,18621.314453125,18642.232421875,18370.001953125,18364.121093749996,19107.46484375,18732.121093749996,17150.623046875,17108.40234375,17717.4140625,18177.484374999996,19625.8359375,18802.998046874996,19201.091796874996,19445.398437499996,18699.765625,19154.23046875,19345.121093749996,19191.630859374996,18321.144531249996,18553.916015625,18264.9921875,18058.904296874996,18803.656249999996,19142.382812499996,19246.64453125,19417.076171875,21310.59765625,22805.162109375,23137.9609375,23869.832031249996,23477.294921874996,22803.082031249996,23783.029296874996,23241.345703124996,23735.949218749996,24664.791015624996,26437.037109374996,26272.294921874996,27084.80859375,27362.4375,28840.953124999996,29001.720703125,29374.15234375,32127.267578125,32782.0234375,31971.914062500004,33992.4296875,36824.36328125,39371.04296875,40797.609375,40254.546875,38356.44140625,35566.65625,33922.9609375,37316.359375,39187.328125,36825.3671875,36178.140625,35791.27734375,36630.07421875,36069.8046875,35547.75,30825.69921875,33005.76171875,32067.642578125,32289.37890625,32366.392578125,32569.849609375,30432.546875000004,33466.09765625,34316.38671875,34269.5234375,33114.359375,33537.17578125,35510.2890625,37472.08984375,36926.06640625,38144.30859375,39266.01171875,38903.44140625,46196.46484375,46481.10546875,44918.18359375,47909.33203125,47504.8515625,47105.515625,48717.2890625,47945.05859375,49199.87109375,52149.0078125,51679.79687499999,55888.1328125,56099.51953125,57539.9453125,54207.3203125,48824.42578125,49705.33203125,47093.8515625,46339.76171875,46188.453125,45137.76953125,49631.2421875,48378.98828125,50538.2421875,48561.16796875,48927.3046875,48912.3828125,51206.69140624999,52246.52343749999,54824.1171875,56008.55078125,57805.12109375,57332.08984374999,61243.0859375,59302.31640625001,55907.19921875001,56804.90234375,58870.89453125,57858.92187499999,58346.65234375,58313.64453125,57523.421875,54529.14453125,54738.94531249999,52774.265625,51704.16015624999,55137.31250000001,55973.51171875,55950.74609375,57750.19921875,58917.69140625,58918.83203125,59095.80859375001,59384.3125,57603.89062500001,58758.5546875,59057.87890625,58192.35937499999,56048.9375,58323.953125,58245.00390625,59793.23437500001,60204.96484375,59893.45312500001,63503.45703125,63109.69531249999,63314.01171875,61572.7890625,60683.8203125,56216.18359375,55724.265625,56473.03125,53906.08984375,51762.27343749999,51093.65234375,50050.8671875,49004.25390625,54021.75390625,55033.11718749999,54824.70312499999,53555.109375,57750.17578125001,57828.05078125,56631.07812499999,57200.29296875,53333.5390625,57424.0078125,56396.515625,57356.40234375,58803.77734375,58232.31640625001,55859.796875,56704.57421875,49150.53515625,49716.19140625,49880.53515625,46760.1875,46456.05859375,43537.51171875,42909.40234375,37002.44140625,40782.73828125,37304.69140625,37536.6328125,34770.58203125,38705.98046875,38402.22265625,39294.19921875,38436.96875,35697.60546875,34616.06640625,35678.12890625,37332.85546875,36684.92578125,37575.1796875,39208.765625,36894.40625,35551.95703125,35862.37890625,33560.70703125,33472.6328125,37345.12109375,36702.59765625,37334.3984375,35552.515625,39097.859375,40218.4765625,40406.26953125,38347.0625,38053.50390625,35787.24609375,35615.87109375,35698.296875,31676.693359375,32505.66015625,33723.02734375,34662.4375,31637.779296875,32186.277343750004,34649.64453125,34434.3359375,35867.77734375,35040.8359375,33572.1171875,33897.046875,34668.546875,35287.78125,33746.00390625,34235.1953125,33855.328125,32877.37109375,33798.01171875,33520.51953125,34240.1875,33155.84765625,32702.025390625,32822.34765625,31780.73046875,31421.5390625,31533.068359375,31796.810546874996,30817.83203125,29807.34765625,32110.693359375,32313.10546875,33581.55078125,34292.4453125,35350.1875,37337.53515625,39406.94140625,39995.90625,40008.421875,42235.546875,41626.1953125,39974.89453125,39201.9453125,38152.98046875,39747.50390625,40869.5546875,42816.5,44555.80078125,43798.1171875,46365.40234375,45585.03125,45593.63671875,44428.2890625,47793.3203125,47096.9453125,47047.00390625,46004.484375,44695.359375,44801.1875,46717.578125,49339.17578125,48905.4921875,49321.65234375,49546.1484375,47706.1171875,48960.7890625,46942.21875,49058.66796875,48902.40234375,48829.83203125,47054.984375,47166.6875,48847.02734375,49327.72265625,50025.375,49944.625,51753.41015625,52633.53515625,46811.12890625,46091.390625,46391.421875,44883.91015625,45201.45703125,46063.26953125,44963.07421875,47092.4921875,48176.34765625,47783.359375,47267.51953125,48278.36328125,47260.21875,42843.80078125,40693.67578125,43574.5078125,44895.09765625,42839.75,42716.59375,43208.5390625,42235.73046875,41034.54296875,41564.36328125,43790.89453125,48116.94140625,47711.48828125,48199.953125,49112.90234375,51514.8125,55361.44921875,53805.984375,53967.84765625,54968.22265625,54771.578125,57484.7890625,56041.05859375001,57401.09765625001,57321.52343750001,61593.94921875,60892.1796875,61553.6171875,62026.078125,64261.99218749999,65992.8359375,62210.171875,60692.26562499999,61393.6171875,60930.8359375,63039.82421875,60363.79296875,58482.38671875,60622.13671875,62227.96484374999,61888.83203124999,61318.95703124999,61004.40625,63226.40234375,62970.046875,61452.23046875,61125.67578125,61527.48046875,63326.98828125,67566.828125,66971.828125,64995.23046875,64949.96093750001,64155.94140625,64469.52734375,65466.83984374999,63557.87109375,60161.24609375,60368.01171875,56942.13671874999,58119.578125,59697.1953125,58730.4765625,56289.28906249999,57569.07421874999,56280.42578125,57274.6796875,53569.76562499999,54815.078125,57248.45703125,57806.56640625001,57005.42578125,57229.82812500001,56477.81640625001,53598.24609375,49200.703125,49368.84765625,50582.625,50700.0859375,50504.796875,47672.12109375,47243.3046875,49362.5078125,50098.3359375,46737.48046875,46612.6328125,48896.72265625,47665.42578125,46202.14453125,46848.77734375,46707.015625,46880.27734375,48936.61328125,48628.51171875,50784.53906250001,50822.1953125,50429.859375,50809.51562500001,50640.41796875001,47588.85546875,46444.7109375,47178.125,46306.4453125,47686.8125,47345.21875,46458.1171875,45897.57421875,43569.00390625,43160.9296875,41557.90234375,41733.94140625,41911.6015625,41821.26171875,42735.85546875,43949.1015625,42591.5703125,43099.69921875,43177.3984375,43113.87890625,42250.55078125,42375.6328125,41744.328125,40680.41796875,36457.31640625,35030.25,36276.8046875,36654.328125,36954.00390625,36852.12109375,37138.234375,37784.33203125,38138.1796875,37917.6015625,38483.125,38743.2734375,36952.984375,37154.6015625,41500.875,41441.1640625,42412.43359375,43840.28515625,44118.4453125,44338.796875,43565.11328125,42407.9375,42244.46875,42197.515625,42586.91796875,44575.203125,43961.859375,40538.01171875,40030.9765625,40122.15625,38431.37890625,37075.28125,38286.02734375,37296.5703125,38332.609375,39214.21875,39105.1484375,37709.78515625,43193.234375,44354.63671875,43924.1171875,42451.7890625,39137.60546875,39400.5859375,38419.984375,38062.0390625,38737.26953125,41982.92578125,39437.4609375,38794.97265625,38904.01171875,37849.6640625,39666.75390625,39338.78515625,41143.9296875,40951.37890625,41801.15625,42190.65234375,41247.82421875,41077.99609375,42358.80859375,42892.95703125,43960.93359375,44348.73046875,44500.828125,46820.4921875,47128.00390625,47465.73046875,47062.6640625,45538.67578125,46281.64453125,45868.94921875,46453.56640625,46622.67578125,45555.9921875,43206.73828125,43503.84765625,42287.6640625,42782.13671875,42207.671875,39521.90234375,40127.18359375,41166.73046875,39935.515625,40553.46484375,40424.484375,39716.953125,40826.21484375,41502.75,41374.37890625,40527.36328125,39740.3203125,39486.73046875,39469.29296875,40458.30859375,38117.4609375,39241.12109375,39773.828125,38609.82421875,37714.875,38469.09375,38529.328125,37750.453125,39698.37109375,36575.140625,36040.921875,35501.953125,34059.265625,30296.953125000004,31022.906249999996,28936.355468749996,29047.751953125,29283.103515625,30101.265624999996,31305.11328125,29862.917968749996,30425.857421875,28720.271484375004,30314.333984375,29200.740234375004,29432.226562500004,30323.722656249996,29098.910156250004,29655.5859375,29562.361328125,29267.224609375,28627.57421875,28814.900390625004,29445.95703125,31726.390624999996,31792.310546875,29799.080078124996,30467.48828125,29704.390625000004,29832.9140625,29906.662109375004,31370.671875,31155.478515625,30214.355468750004,30111.998046875004,29083.8046875,28360.810546875,26762.6484375,22487.388671874996,22206.79296875,22572.83984375,20381.650390625,20471.482421875,19017.642578124996,20553.271484374996,20599.537109375,20710.597656249996,19987.029296875,21085.876953125,21231.656249999996,21502.337890624996,21027.294921875,20735.478515624996,20280.634765624996,20104.023437499996,19784.7265625,19269.3671875,19242.255859375,19297.076171874996,20231.26171875,20190.115234375,20548.24609375,21637.587890624996,21731.1171875,21592.207031249996,20860.44921875,19970.556640625,19323.914062499996,20212.074218749996,20569.919921875,20836.328124999996,21190.316406249996,20779.343749999996,22485.689453124996,23389.433593749996,23231.732421874996,23164.628906249996,22714.978515624996,22465.478515625,22609.1640625,21361.701171875,21239.753906249996,22930.548828125,23843.88671875,23804.632812499996,23656.20703125,23336.896484375,23314.19921875,22978.117187499996,22846.5078125,22630.957031249996,23289.314453125,22961.279296875,23175.890625,23809.486328124996,23164.318359374996,23947.642578124996,23957.529296875,24402.818359375,24424.068359374996,24319.333984374996,24136.97265625,23883.291015625,23335.998046875,23212.73828125,20877.552734375,21166.060546875,21534.121093749996,21398.908203125,21528.087890624996,21395.01953125,21600.904296875,20260.019531249996,20041.738281249996,19616.814453124996,20297.994140625,19796.80859375,20049.763671875,20127.140625,19969.771484374996,19832.087890625,19986.712890624996,19812.37109375,18837.66796875,19290.32421875,19329.833984375,21381.152343749996,21680.5390625,21769.255859374996,22370.44921875,20296.70703125,20241.08984375,19701.210937499996,19772.583984375,20127.576171875,19419.505859375,19544.12890625,18890.7890625,18547.400390624996,19413.55078125,19297.638671875,18937.011718749996,18802.09765625,19222.671875,19110.546875,19426.720703125,19573.050781249996,19431.7890625,19312.095703124996,19044.107421874996,19623.580078125,20336.843749999996,20160.716796875,19955.443359375,19546.849609375,19416.568359375,19446.425781249996,19141.484374999996,19051.41796875,19157.4453125,19382.904296875,19185.656249999996,19067.634765624996,19268.09375,19550.757812499996,19334.416015624996,19139.535156249996,19053.740234375,19172.46875,19208.189453125,19567.0078125,19345.572265624996,20095.857421875,20770.441406249996,20285.835937499996,20595.351562499996,20818.4765625,20635.603515625,20495.773437499996,20485.2734375,20159.50390625,20209.98828125,21147.230468749996,21282.691406249996,20926.486328125,20602.81640625,18541.271484375,15880.7802734375,17586.771484375,17034.29296875,16799.185546875,16353.365234375,16618.19921875,16884.61328125,16669.439453125,16687.517578125,16697.77734375,16711.546875,16291.83203125,15787.2841796875,16189.76953125,16610.70703125,16604.46484375,16521.841796875,16464.28125,16444.626953125,16217.322265625,16444.982421875,17168.56640625,16967.1328125,17088.66015625,16908.236328125,17130.486328125,16974.826171875,17089.50390625,16848.126953125,17233.474609375,17133.15234375,17128.724609375,17104.193359375,17206.4375,17781.318359374996,17815.650390625,17364.865234375,16647.484375,16795.091796875,16757.9765625,16439.6796875,16906.3046875,16817.53515625,16830.341796875,16796.953125,16847.755859375,16841.986328125,16919.8046875,16717.173828125,16552.572265625,16642.341796875,16602.5859375,16547.49609375,16625.080078125,16688.470703125,16679.857421875,16863.23828125,16836.736328125,16951.96875,16955.078125,17091.14453125,17196.5546875,17446.29296875,17934.896484375,18869.587890624996,19909.574218749996,20976.298828124996,20880.798828125,21169.632812499996,21161.51953125,20688.78125,21086.792968749996,22676.552734374996,22777.625,22720.416015624996,22934.431640624996,22636.46875,23117.859375,23032.777343749996,23078.728515625,23031.08984375,23774.56640625,22840.138671875,23139.283203125,23723.769531249996,23471.87109375,23449.322265625,23331.84765625,22955.666015625,22760.109374999996,23264.291015624996,22939.3984375,21819.0390625,21651.183593749996,21870.875,21788.203124999996,21808.1015625,22220.8046875,24307.841796875,23623.474609375,24565.6015625,24641.27734375,24327.642578124996,24829.1484375,24436.353515625,24188.843749999996,23947.492187499996,23198.126953124996,23175.374999999996,23561.212890625,23522.87109375,23147.353515625,23646.55078125,23475.466796874996,22362.679687499996,22353.349609375,22435.513671874996,22429.757812499996,22219.76953125,21718.080078124996,20363.021484374996,20187.244140624996,20632.410156249996,22163.94921875,24197.533203124996,24746.074218749996,24375.960937499996,25052.7890625,27423.9296875,26965.87890625,28038.675781250004,27767.236328125,28175.81640625,27307.4375,28333.97265625,27493.285156249996,27494.707031249996,27994.330078125,27139.888671875,27268.130859375,28348.44140625,28033.5625,28478.484374999996,28411.035156250004,28199.30859375,27790.220703125004,28168.08984375,28177.984375,28044.140624999996,27925.859375,27947.794921875,28333.05078125,29652.980468750004,30235.058593749996,30139.052734375,30399.066406250004,30485.69921875,30318.496093749996,30315.35546875,29445.044921875,30397.552734375004,28822.6796875,28245.988281250004,27276.91015625,27817.5,27591.384765625,27525.339843749996,28307.597656250004,28422.701171875,29473.787109375,29340.26171875,29248.48828125,29268.806640624996,28091.568359375004,28680.537109375,29006.30859375,28847.7109375,29534.384765625,28904.623046875,28454.978515625,27694.2734375,27658.775390625,27621.755859375,27000.789062499996,26804.990234375,26784.078124999996,26930.638671875,27192.693359375,27036.650390624996,27398.802734375,26832.208984375,26890.12890625,27129.5859375,26753.826171875,26851.27734375,27225.726562500004,26334.818359375,26476.207031249996,26719.291015625004,26868.353515624996,28085.646484375,27745.884765624996,27702.349609375,27219.658203125,26819.972656249996,27249.589843750004,27075.12890625,27119.066406249996,25760.09765625,27238.783203125,26345.998046874996,26508.216796875,26480.375,25851.240234375,25940.16796875,25902.499999999996,25918.728515624996,25124.675781249996,25576.394531250004,26327.462890625,26510.675781249996,26336.212890625,26851.029296875,28327.48828125,30027.296875,29912.28125,30695.468750000004,30548.6953125,30480.26171875,30271.130859375,30688.1640625,30086.246093750004,30445.3515625,30477.251953125,30590.078124999996,30620.76953125,31156.439453125,30777.58203125,30514.166015625,29909.337890625,30342.265625,30292.541015625,30171.234375,30414.470703125,30620.951171875,30391.646484375004,31476.048828124996,30334.068359375,30295.806640624996,30249.132812500004,30145.888671875,29856.5625,29913.923828124996,29792.015625,29908.744140625,29771.802734375,30084.539062499996,29176.916015625,29227.390625,29354.972656250004,29210.689453124996,29319.24609375,29356.917968750004,29275.30859375,29230.111328125,29675.732421874996,29151.958984375004,29178.6796875,29074.091796875,29042.126953125,29041.85546875,29180.578124999996,29765.4921875,29561.494140625,29429.591796874996,29397.71484375,29415.964843750004,29282.9140625,29408.443359375,29170.347656249996,28701.779296874996,26664.55078125,26049.556640625,26096.205078125,26189.583984375,26124.140625,26031.65625,26431.640625,26162.373046875004,26047.66796875,26008.462890625,26089.693359375004,26106.150390625,27727.392578125,27297.265625,25931.472656250004,25800.724609375,25868.798828125,25969.56640625,25812.416015624996,25779.982421875,25753.236328125,26240.1953125,25905.654296874996,25895.677734375,25832.2265625,25162.654296875,25833.343749999996,26228.32421875,26539.673828124996,26608.693359375,26568.28125,26534.1875,26754.281249999996,27211.1171875,27132.0078125,26567.6328125,26579.568359375,26579.390625,26256.826171875,26298.480468749996,26217.25,26352.716796875,27021.546875,26911.720703125,26967.916015625,27983.75,27530.785156250004,27429.978515625,27799.394531249996,27415.912109375004,27946.59765625,27968.839843750004,27935.08984375,27583.677734374996,27391.019531249996,26873.3203125,26756.798828125,26862.375,26861.707031249996,27159.65234375,28519.466796875,28415.748046875,28328.341796875004,28719.806640625004,29682.94921875,29918.412109375004,29993.896484374996,33086.234375,33901.52734375,34502.8203125,34156.6484375,33909.80078125,34089.57421875,34538.48046875,34502.36328125,34667.78125,35437.25390625,34938.2421875,34732.32421875,35082.1953125,35049.35546875,35037.37109375,35443.5625,35655.27734375,36693.125,37313.96875,37138.05078125,37054.51953125,36502.35546875,35537.640625,37880.58203125,36154.76953125,36596.68359375,36585.703125,37386.546875,37476.95703125,35813.8125,37432.33984375,37289.62109375,37720.28125,37796.79296875,37479.12109375,37254.16796875,37831.0859375,37858.4921875,37712.74609375,38688.75,39476.33203125,39978.390625,41980.09765625,44080.6484375,43746.4453125,43292.6640625,44166.6015625,43725.984375,43779.69921875,41243.83203125,41450.22265625,42890.7421875,43023.97265625,41929.7578125,42240.1171875,41364.6640625,42623.5390625,42270.52734375,43652.25,43869.15234375,43997.90234375,43739.54296875,43016.1171875,43613.140625,42520.40234375,43442.85546875,42627.85546875,42099.40234375,42156.90234375,42265.1875,44167.33203125,44957.96875,42848.17578125,44179.921875,44162.69140625,43989.1953125,43943.09765625,46970.50390625,46139.73046875,46627.77734375,46368.5859375,42853.16796875,42842.3828125,41796.26953125,42511.96875,43154.9453125,42742.65234375,41262.05859375,41618.40625,41665.5859375,41545.78515625,39507.3671875,39845.55078125,40077.07421875,39933.80859375,41816.87109375,42120.0546875,42035.59375,43288.24609375,42952.609375,42582.60546875,43075.7734375,43185.859375,42992.25,42583.58203125,42658.66796875,43084.671875,44318.22265625,45301.56640625,47147.19921875,47771.27734375,48293.91796875,49958.22265625,49742.44140625,51826.6953125,51938.55468749999,52160.20312499999,51662.99609375001,52122.546875,51779.14453125,52284.875,51839.1796875,51304.97265625,50731.94921875,51571.10156249999,51733.23828125,54522.40234375,57085.37109375,62504.7890625,61198.3828125,62440.6328125,62029.84765625,63167.37109375,68330.4140625,63801.19921875001,66106.8046875,66925.484375,68300.09375,68498.8828125,69019.7890625,72123.90625,71481.2890625,73083.5,71396.59375,69403.7734375,65315.11718749999,68390.625,67548.59375,61912.77343750001,67913.671875,65491.39062499999,63778.76171875,64062.203125,67234.171875,69958.8125,69987.8359375,69455.34375,70744.953125,69892.828125,69645.3046875,71333.6484375,69702.1484375,65446.97265625,65980.8125,68508.84375,67837.640625,68896.109375,69362.5546875,71631.359375,69139.015625,70587.8828125,70060.609375,67195.8671875,63821.47265625001,65738.7265625,63426.21093750001,63811.86328125,61276.69140625,63512.75390624999,63843.5703125,64994.44140625,64926.64453124999,66837.6796875,66407.2734375,64276.89843750001,64481.70703125,63755.32031250001,63419.140625,63113.23046875001,63841.12109374999,60636.85546875,58254.01171874999,59123.43359374999,62889.83593749999,63891.47265625,64031.13281250001,63161.94921875001,62334.81640625001,61187.94140625,63049.9609375,60792.77734375,60793.7109375,61448.39453125,62901.44921875,61552.7890625,66267.4921875,65231.58203124999,67051.875,66940.8046875,66278.3671875,71448.1953125,70136.53125,69122.3359375,67929.5625,68526.1015625,69265.9453125,68518.09375,69394.5546875,68296.21875,67578.09375,68364.9921875,67491.4140625,67706.9375,67751.6015625,68804.78125,70567.765625,71082.8203125,70757.1640625,69342.5859375,69305.7734375,69647.9921875,69512.28125,67332.03125,68241.1875,66756.3984375,66011.09375,66191.0,66639.046875,66490.296875,65140.74609375,64960.296875,64828.65625000001,64096.19921875,64252.578125,63180.79687499999,60277.41406250001,61804.640625,60811.27734375,61604.80078125001,60320.13671875001,60887.37890625,62678.29296875,62851.98046875,62029.015625,60173.921875,56977.703125,56662.37500000001,58303.53906250001,55849.109375,56705.09765625001,58009.22656249999,57742.49609374999,57344.9140625,57899.46484375,59231.95312500001,60787.79296874999,64870.15234375,65097.1484375,64118.79296874999,63974.06640625001,66710.15625,67163.6484375,68154.5234375,67585.25,65927.671875,65372.13281250001,65777.2265625,67912.0625,67813.3359375,68255.8671875,66819.9140625,66201.015625,64619.25000000001,65357.5,61415.06640625,60680.09374999999,58116.9765625,53991.45703125,56034.31640625001,55027.4609375,61710.13671875,60880.11328125001,60945.8125,58719.484375,59354.515625,60609.56640625,58737.26953125001,57560.09765625,58894.10546875,59478.97265625,58483.96484375,59493.453125,59012.79296875,61175.19140625,60381.9140625,64094.35546875,64178.9921875,64333.54296875,62880.66015625,59504.1328125,59027.625,59388.1796875,59119.47656249999,58969.8984375,57325.48828125,59112.48046875,57431.02343750001,57971.5390625,56160.48828125,53948.75390625,54139.6875,54841.56640625,57019.53515625,57648.71093749999,57343.171875,58127.01171874999,60571.30078125,60005.12109375001,59182.8359375,58192.5078125,60308.5390625,61649.67968749999,62940.45703125,63192.97656249999,63394.83984375,63648.71093749999,63329.80078124999,64301.96875,63143.14453124999,65181.01953125001,65790.6640625,65887.6484375,65635.3046875,63329.49999999999,60837.00781249999,60632.78515625,60759.40234375,62067.4765625,62089.94921875,62818.953125,62236.66015625,62131.96875,60582.10156250001,60274.49999999999,62445.08984375,63193.0234375,62851.37499999999,66046.125,67041.109375,67612.71875,67399.8359375,68418.7890625,68362.734375,69001.703125,67367.8515625,67361.40625,66432.1953125,68161.0546875,66642.4140625,67014.6953125,67929.296875,69907.7578125,72720.4921875,72339.5390625,70215.1875,69482.46875,69289.2734375,68741.1171875,67811.5078125,69359.5625,75639.078125,75904.859375,76545.4765625,76778.8671875,80474.1875,88701.484375,87955.8125,90584.1640625,87250.4296875,91066.0078125,90558.4765625,89845.8515625,90542.640625,92343.7890625,94339.4921875,98504.7265625,98997.6640625,97777.28125,98013.8203125,93102.296875,91985.3203125,95962.53125,95652.46875,97461.5234375,96449.0546875,97279.7890625,95865.3046875,96002.1640625,98768.53125,96593.5703125,99920.7109375,99923.3359375,101236.015625,97432.71875,96675.4296875,101173.03125,100043.0,101459.25781250001,101372.96875,104298.6953125,106029.71874999999,106140.6015625,100041.5390625,97490.953125,97755.9296875,97224.7265625,95104.9375,94686.2421875,98676.09375,99299.1953125,95795.515625,94164.859375,95163.9296875,93530.2265625,92643.2109375,93429.203125,94419.7578125,96886.875,98107.4296875,98236.2265625,98314.9609375,102078.0859375,96922.703125,95043.5234375,92484.0390625,94701.453125,94566.59375,94488.4375,94516.5234375,96534.046875,100504.4921875,99756.90625,104462.0390625,104408.0703125,101089.609375,102016.6640625,106146.265625,103653.0703125,103960.171875,104819.48437499999,104714.64843750001,102682.5,102087.68750000001,101332.4765625,103703.2109375,104735.30468750001,102405.0234375,100655.90625,97688.9765625,101405.421875,97871.8203125,96615.4453125,96593.296875,96529.0859375,96482.453125,96500.09375,96976.8203125],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Predicted Prices\",\"y\":[8840.19597464367,8831.419294375417,8780.538595551541,8713.486281010511,8636.375045100456,8586.059708313958,8545.184786827595,8516.413889653464,8573.903670694286,8671.53340257288,8768.458316444878,8792.232167024638,8764.169856937355,8730.555314734997,8730.958229413634,8812.423883735573,8933.95938400265,9068.840443616042,9178.489018593238,9263.586558643292,9361.699486609832,9552.099852109772,9828.995709524188,10090.604931263959,10249.103047718745,10290.695225176529,10277.287476006102,10257.434599609569,10305.900448717792,10467.111926043966,10624.997943866285,10557.297347522395,10348.556260621895,10159.808636811224,10064.371755821605,10102.505802150761,10221.365255440916,10352.90314087328,10459.527782766556,10541.794743549472,10591.755032976496,10594.428441597243,10487.517726513055,10358.199075531411,10258.379884936065,10144.00337569429,10037.807655133372,9972.98965136406,9976.261966836402,10084.396502277501,10254.614196859104,10438.993079752945,10593.451495996655,10730.757581842372,10806.694749254806,10842.11053490825,10853.779984267752,10854.562822235508,10840.247101633053,10805.19465531563,10796.522755478305,10804.077123021747,10831.412754436988,10806.603537512776,10766.7085768912,10743.942955278097,10718.336577881182,10664.229169455979,10661.257249679496,10685.34167247179,10714.303285106529,10716.231169653987,10704.344244364107,10692.493879152644,10701.080597776338,10758.994023437168,10791.831381292144,10769.961670054272,10715.403479631266,10641.306377196484,10582.288987930668,10550.722187027692,10545.566839062856,10565.09708219006,10584.699691658047,10599.846117547422,10607.06993675301,10597.164793858152,10600.075277626573,10634.27214272743,10678.650424305411,10723.98869062286,10746.786349418082,10759.903502501296,10759.164008956406,10743.566801069228,10721.991454998817,10706.902319129229,10686.35555505882,10668.6431393358,10660.919163181145,10655.794344766327,10652.12778349992,10662.927706045935,10719.411896476275,10805.803361775874,10893.324043518129,10971.56072679059,11054.841193252512,11236.82746400417,11526.534802093593,11840.662764933535,12086.83647232882,12251.291997025468,12380.6679380269,12424.025175765377,12394.111493459694,12323.25655385526,12292.295821056354,12343.79804121332,12409.637842638187,12484.864161515625,12536.18772726878,12564.58774694639,12530.941921377964,12475.024600617102,12436.59128931616,12473.78155795092,12547.769357058476,12627.077589845781,12711.166524021151,12789.560377940004,12799.347925531089,12771.07906984615,12692.041457020736,12597.721731416977,12526.777087702614,12492.546300879514,12431.166829021533,12377.765746235027,12328.875498690746,12318.844468515565,12347.107293672105,12397.788608150799,12454.091882729177,12522.303189446831,12541.17874333475,12356.19831523314,12067.571195543134,11733.22965532681,11493.630731518206,11378.341351040079,11343.722348951313,11374.548148677299,11373.237262566632,11401.579992224455,11471.800218714925,11522.774013820956,11570.30890010763,11642.614935607999,11726.295301484359,11772.465780720828,11815.820003195106,11855.834066752555,11855.548370469667,11758.76442019899,11623.017979760385,11470.027243365708,11332.791262412626,11323.548723825574,11388.32413698807,11481.669178439392,11603.0584383928,11710.442549956286,11782.082212068599,11795.67954597556,11718.684774645204,11620.63818249105,11486.591597244827,11473.021400715661,11508.09043098618,11546.126481228857,11529.615648289277,11633.300030680162,11773.255026125007,11902.104049707603,11981.590936898721,12012.342862651823,12004.76851898306,11987.726245728345,11962.049763438796,11957.137898058058,11983.692576045667,12049.493179035942,12160.42927182626,12388.704370934212,12754.596881916463,13118.932762755801,13412.735583443988,13591.732219741993,13638.614301329508,13672.273695586831,13689.554174713296,13656.336516661177,13714.960037040953,13844.213106026262,13995.330609537094,14049.28499330418,14065.850854815399,14096.026111290375,14362.477470426275,14922.82060054424,15375.583626925763,15702.615412906816,15851.203863570445,15891.443318126796,15909.905780819405,16018.921904130788,16247.681706958785,16452.66690539097,16564.353798966928,16674.125245960247,16979.51045025418,17468.284023152468,17999.723584471532,18514.745786041196,18983.27110602481,19208.241475570278,19248.65506163414,19276.576408120105,19371.961275802285,19081.11190639736,18613.471566456254,18266.262386269213,18192.07181064425,18507.658407540628,18969.71900108084,19403.383344023692,19787.549109870044,19934.34423215111,19905.66906961531,19850.580192693218,19768.64038807593,19554.407374344344,19264.910350932827,19041.434044798138,18837.19813951949,18833.86928784341,19071.375618297665,19431.88965176433,19782.444267580548,20302.46577719273,21244.00011502439,22485.842706054333,23604.855494659627,24248.8872826305,24260.621183262265,24017.42052636754,23720.828586294956,23527.560704541436,23639.093819643225,24405.486536455748,25601.684058035957,26750.062504678703,27535.228256396367,28287.382895659568,28966.90283562467,29505.40188425046,30417.817815709015,31900.067421352986,32750.676467253303,33498.054928024736,34930.48332330288,37295.28266851086,39847.556065313314,41750.74258371469,41949.70177664448,39922.449197496724,37364.43473765871,35836.455635167425,36354.30313929188,37205.27703216032,37673.17366955837,37420.97395666069,37074.56532911857,36876.86048609571,36377.60811628864,34870.09361191833,33252.73303276577,32388.12411518788,32069.962482665258,32360.121356568532,32670.964942879335,32435.554221043625,32474.731548785756,33528.98852344585,34730.49894054531,35265.304290526896,35219.21597723986,35345.61886774073,36176.61663528849,37246.29367106405,38222.361829770845,39358.94456286574,40028.24577245925,41878.48123625587,44839.416405260185,46913.59252587819,48245.18747106593,48984.21666526937,48946.88769448144,48884.03752751023,48639.81620368402,48748.000867892435,49855.16366819429,51281.04894015985,53131.971884193925,55162.79152088951,56828.24254875747,56455.97200017755,53779.66872579911,51024.775605328854,48954.872114081314,47482.20803332707,47607.52241345198,47755.02913808409,48600.08708259341,49509.30381873646,50459.66383487961,50638.47503242936,49966.54752771821,49142.216509315214,49141.706929665525,49923.655394482485,51691.18819181939,53922.21835608749,56053.93476342166,57407.31197718549,58808.921357082094,59849.19353638561,59191.40159024101,57552.46076521371,56748.04858211161,56687.57444332948,57238.17977771565,58104.47121271678,58336.67670870858,57538.95238160063,56230.900619281405,54951.716996406736,53541.0105644207,53279.88566949295,54331.905318117126,55726.88111677516,57102.96930048848,58474.04008457409,59420.01289130719,59860.62138770114,60018.386041139434,59564.05809263845,58960.11273456581,58736.150970895025,58647.76754668373,58095.09374105681,57810.205548973994,58066.34018165182,58874.86518512184,59847.921094893485,60504.651668039915,61525.82516390476,62733.233437778144,63543.18164595025,63412.5121566073,62649.222146655906,60354.45532774198,57929.591919682985,56316.14213819698,55362.78494943967,54238.1215557248,52776.67727305633,51600.9533953897,50517.050343121606,50945.027897784494,52865.48584951973,54840.800502833714,55582.48011426838,56247.99113676331,57213.569191320574,57561.44022199937,57613.097728262605,56503.66141875444,55823.49621225068,56008.04847283598,56693.74367388135,57890.17035554071,58768.59521368001,58531.71605817922,57722.774948249695,55474.00598469683,52483.059891116405,50550.61023366035,49113.812720557355,47922.481835394894,46488.4523348269,45260.67293602272,42425.26321563727,40589.298997296166,39472.19788619326,38902.29184566706,37927.423762111604,37880.36453375238,38618.66403401957,39733.060453135666,40374.59107945318,39732.36091184142,38064.51877946337,36457.67544183813,35781.21901029957,35970.514281470445,36664.103444189706,37863.66298535219,38490.69622139924,38153.14945533901,37364.0276769918,36147.84799956251,34579.96489065705,34332.61673785612,35416.435362726625,36848.58032317003,37590.103140866326,38275.12895325944,39526.798290315055,40820.18983809705,41128.86544944852,40598.10658402246,39182.75061464103,37473.79837016907,36026.42934078316,34430.83595388569,32882.47366540681,32542.34884845614,33245.93059673204,33763.262490678404,33608.44375036287,33834.962458074704,34459.09802522618,35365.6583136096,36004.63501115027,35826.414805383596,35171.98789408547,34762.433603680605,34873.06364715469,34849.64710538255,34770.8009618338,34685.185550157505,34229.22935898887,33859.524800242594,33765.87069421084,33976.70399755766,34088.543161975686,33877.12489737419,33479.02559566265,32949.90703396103,32362.250133093272,31945.552681800153,31829.13031579912,31692.53884756865,31240.240171873826,31177.058325840684,31703.3998292147,32625.37716344939,33795.42028336445,34976.25201858478,36585.21207167796,38357.78940602037,40126.63685854818,41321.18503061135,42023.28929942957,42517.575529100315,42316.954925596685,41430.554693650105,40204.484949646954,39406.81791255879,39413.31580690839,40545.01079673614,42570.33662206051,44425.167348496936,45874.31538376046,46774.64312104383,47073.37137580165,46602.94493174035,46484.92749097786,46824.120591290935,47100.23134928945,47158.567665754235,46697.57296810849,46028.522025443526,45821.54826027187,46827.38913768303,48323.93712616031,49518.93155732498,50302.457005318196,50149.05543917636,49589.54602960951,48746.523388434754,48261.23772239906,48434.16915476008,48814.46332137447,48753.093649125134,48377.515355718526,48301.83222431489,48869.66677833613,49706.743318509776,50481.02698173199,51271.20711781318,52158.85868440249,51203.71645923917,49152.59504868937,47448.64512752413,46197.63010279977,45569.98175285605,45634.43905564552,45787.617492236604,46347.59125249015,47437.229337265395,48375.434823420976,48721.080589120655,48806.72313817474,48558.09952861749,47080.436139820726,44585.04871740317,42941.165874353144,42932.91008097533,43341.73467744395,43656.90818314502,43841.13177993259,43916.34151485693,43456.12777063882,42875.984908155835,42924.7055470889,44459.71021516467,46727.680381138984,48561.47059399236,49620.621342447994,50638.52930718495,52330.49053789359,53938.09975262546,54866.924751856866,55202.626176227095,55253.34593532433,55679.318339294645,56034.64913360263,56360.08056810988,56934.0361084558,58390.095129264235,60101.65557726863,61112.91488444602,61625.74498894276,62363.580138523255,63704.43194480299,64141.705558992464,63193.90550215456,61852.82453579567,60766.39469212853,60790.896729012435,60963.08037585201,60477.38764914941,60220.84897106887,60815.971666094214,61605.24722291559,61971.638006306785,61872.21268459748,62141.04761008296,62530.348370860804,62505.19503690984,62131.826932161355,61631.00964023482,61799.26741308685,63470.38110712119,65706.17538882427,66687.74037416647,66452.74877405447,65420.89521239553,64483.23247379583,64181.68193174742,64110.72070407996,62968.66526646325,61481.37413907797,59791.27031111613,58389.60062593554,58200.62798803399,58766.496589797265,58675.88790060685,58267.34975423717,57762.11208499439,57733.231884493034,57099.88770047675,56090.045567473055,55807.262029801386,56570.17211646366,57390.00436068123,57961.656239178854,57896.930577875646,56781.692900047936,53686.17744403412,50866.386792201294,49385.6116507508,49514.80366063607,50204.675002597534,50330.028581157036,49817.00851501574,49396.19221282631,49599.49640146736,49243.23389052178,48503.64385717726,48324.62159113356,48436.587396647956,48099.07982902232,47651.61462183905,47453.99722147797,47317.45399747469,47808.85461930666,48654.7116088288,49727.25013032954,50875.47781376233,51463.60208058099,51498.47964157485,51365.90450525791,50373.35491243747,48779.1696539211,47532.00512157931,46862.12799652372,46922.91873804678,47401.006968438625,47652.72423906441,47587.26888382496,46766.016450169496,45482.01141984007,43982.300419217674,42929.738023037615,42548.27091864968,42390.15046403586,42705.706908290245,43503.621197042754,44117.33299585708,44340.2032338877,44362.28401361953,44190.21494681953,43707.474163768085,43133.94076041007,42622.033326758334,42178.273879276065,40709.801015758916,38528.11565011315,36905.35473277955,36032.44479286086,36291.127323787136,37183.56109939661,37930.44807210355,38531.38721176944,39053.579711605154,39271.28781732649,39237.0223549651,39304.32305189455,39010.419219855685,38406.71206765479,38857.77448502788,40289.95864390588,41854.38324477468,43346.46261170853,44540.20872349467,45199.6952481193,45321.69585288913,44694.87971586443,43770.48715500606,42984.272091347026,42558.70976330782,42976.5349234115,43777.90470493646,43664.65741213734,42667.10549601019,41564.85154582487,40397.59453003001,39203.56799867333,38446.84221888392,38244.02047251476,38222.06934914351,38921.38751384153,39837.916265668,40159.883161610196,41015.78399618037,42837.175442646025,44520.43462087534,45045.55795751282,43861.798400754924,41941.2710979431,40132.5648679641,38811.19769894055,38361.58863891155,39290.654859278904,40423.16698570468,40889.39618200049,40826.54903029348,40242.47727855874,39798.21428195518,39682.271342961845,40179.045165597054,41058.410786166554,41935.466714359354,42734.614246169396,43001.154555599176,42724.024638301285,42576.05657824667,42763.250210270606,43381.816584445885,44263.265752577136,45021.70721769598,45911.39107235565,46995.17351405576,47882.06221880263,48238.89762994609,47825.03151173616,47070.48275269868,46585.061399774015,46474.600211095094,46556.36211512462,46631.30047627102,45976.41222955039,45029.96301107379,44059.38263320981,43359.57901097569,42989.16586514257,42204.34385354363,41374.88990126594,41096.011130732426,41084.70388998487,41190.77183872534,41337.78104476456,41279.58041518874,41180.59833731674,41492.90131150003,41972.20469336421,42217.25521484524,41917.80631194377,41325.02044667292,40722.061079993466,40390.09255270203,40058.42555183053,39835.56134432831,40024.69077596822,40147.11351772596,39897.003382918425,39563.21966657898,39417.19343666875,39246.99383367234,39437.609790562536,39258.33424232609,38491.70633490602,37493.0689236671,36310.49436573955,34407.214374147996,32641.177147853974,31119.52708491712,29898.881801208598,29610.188345706207,29914.672739820584,30686.526100098126,31190.8591900811,31333.258057159517,30906.866531465494,30414.244727632788,29996.087888467184,29679.92839137293,29679.388659081247,29683.501479449158,29657.947115359682,29724.06884398719,29644.9091129617,29421.975554382894,29201.137604422634,29261.862010133307,29929.571160229563,31062.16771405292,31596.713751313364,31584.725060856756,31181.1530546234,30637.1451183054,30190.288994490402,30239.721235774516,30528.58957660047,30808.891551836074,30907.656530685723,30586.250473884604,29895.715773799282,28791.95117625009,26638.856545742543,24187.73923229982,22421.32509798088,21591.421871337487,21156.25894208756,20687.414004098813,20476.190224038175,20647.336619993177,21065.971378534377,21321.094390073325,21492.708151979226,21684.307084998407,21867.89446103992,21944.892247634474,21828.317610791375,21542.677110290824,21156.650926433475,20710.74611847337,20365.442585260316,20135.175904148695,20037.58537070469,20151.75533434881,20421.231003476656,20775.985882322566,21285.31526508386,21837.986055446585,22304.29666387578,22413.058751178367,22085.154800027653,21421.72883272657,20812.567067589058,20585.207101165608,20753.404568733647,21148.509713095234,21531.149755256716,21956.7950030614,22570.665103246196,23255.941182567854,23682.78499789178,23796.180038636667,23575.905943081685,23241.694059225934,22723.20430382702,22132.697978290875,21939.341146243474,22350.568908278976,23080.77543943512,23763.981539883927,24100.789566215317,24049.70646778206,23748.14687994105,23399.900448869434,23075.357009668893,22934.97836960404,22982.70698661552,23099.922367100982,23320.896003950213,23491.60367896421,23616.578841866693,23840.006903774192,24086.770095320448,24424.573158837622,24644.878914666697,24642.097333442798,24447.275082994514,24089.928584569265,23725.741959307794,23046.286847522977,22289.636449338548,21815.829894109775,21600.781251412263,21613.5689868817,21773.06590960252,21998.585057232267,21968.028369836073,21677.329763641115,21254.18718512189,20914.92925662853,20718.576759599076,20676.50930112187,20731.248407396837,20824.009995225584,20862.98680789846,20842.71669431834,20776.959812658955,20531.062001937913,20249.656440005347,20091.189230008604,20344.56489594403,21079.728521443903,21881.21288300844,22515.959164877422,22611.58525369555,22187.53658828442,21532.23826563268,20896.062748533208,20529.162385492324,20286.04464836308,20053.04010735845,19878.169860117254,19681.830931776698,19637.343723779515,19735.71068775485,19876.19184680248,19920.15138356478,19863.29254655767,19843.22294804681,19867.951129745663,20006.704542407067,20187.682207284044,20350.94519498985,20316.43549622831,20189.565239776537,20218.397196050704,20377.986084329605,20544.694011383166,20582.945653016097,20506.18607229329,20358.727591888368,20130.70577497316,19894.461332586332,19763.442072596255,19675.57576619515,19772.85271216242,19951.83427213943,20103.624179566716,20210.92084846842,20255.388457248308,20238.964313154458,20149.374783263425,20016.464952620358,19996.28680459832,20061.87181619834,20131.346518615523,20268.541039685864,20619.75901362594,21039.486805439403,21400.86471209918,21685.30513744839,21829.08198026591,21723.047199431632,21537.738107532292,21275.97096133008,21067.76093783669,21048.369773770683,21367.42090923211,21685.466454083056,21733.124212385854,21040.00241561749,19733.232140582986,18608.252144127182,17978.392097893244,17680.585006348323,17476.668719074805,17204.41825306751,17074.551577817634,17009.702290682253,16968.26502242271,16946.829509229545,16956.553736272443,16869.748049053458,16558.97532145134,16289.080530599786,16242.628124160678,16430.01247164522,16694.9298151197,16914.355113802565,17027.727540065956,16977.212818934277,16907.922047633256,16979.618999765356,17205.053719997522,17462.154744851243,17662.32758162127,17783.366317111504,17820.520402575872,17813.711936013744,17761.428762429146,17740.564641801757,17783.48391241528,17875.738935858535,17950.42401481219,17967.86731820542,18057.076924807334,18292.854001243264,18475.8669694548,18401.736699113826,18181.703824694792,17943.704498543942,17670.5125162904,17482.62160442037,17465.522794961915,17516.253861299898,17605.325519564693,17707.24898766316,17766.072269296143,17796.14802205254,17764.445534260594,17666.829371070897,17571.957852118656,17493.34991444165,17466.09871042399,17479.448038850554,17514.560036635914,17554.35700117101,17608.90162290512,17697.042318348365,17786.1554364959,17893.654128098962,17982.20339184122,18050.794621847992,18158.887320498325,18354.63374942371,18735.88827768559,19402.071704096976,20312.3256911246,21275.854873658405,21965.30407863196,22303.940862700256,22251.56270829338,22045.1467669986,22064.67211032148,22413.78090695411,22814.027076775586,23139.99372067822,23290.948399922287,23346.466951992785,23367.237599429995,23350.84059271394,23331.892672485235,23385.748306349793,23386.030233552432,23340.19671009024,23380.745983043074,23513.701042649132,23626.07692409464,23690.359341560543,23600.432102079183,23385.7091079152,23230.7456349181,23163.76757125798,22953.596118402915,22633.46401072602,22394.621918231453,22264.46803906659,22191.545382037453,22247.90971571587,22714.59572453788,23428.04643253825,24103.800307518366,24810.32796891703,25181.895960938506,25207.426202914387,25035.993356860447,24673.63699696798,24292.163862051646,23832.42652957703,23484.94446797995,23308.614832866268,23289.856874282123,23312.86032485895,23397.109821852937,23514.91317885727,23406.315423453547,23150.68132463281,22900.364644227622,22724.517451385836,22597.647194934063,22461.361776019825,22091.476301421586,21538.755759199572,21193.5426839125,21348.471481371307,22191.3207448546,23542.587273678888,24787.999937520857,25737.238275381853,26614.643974221777,27400.026824961795,28020.167167049134,28364.061064248992,28508.558555210155,28345.517189423,28190.116503116995,27947.56262039303,27669.211521094403,27533.111540928425,27317.85484500116,27079.272065227735,27191.298176026117,27563.423991924472,27974.34118174744,28378.998683092563,28546.632296255324,28399.12858688741,28228.591274300677,28160.81416562773,28121.22073142609,28093.609957152657,28088.915190794272,28162.84946896229,28562.251364807948,29350.02230479385,30125.605546885985,30689.4297995221,31034.382054455928,31147.774079936615,31058.950427152216,30663.996046000597,30329.940955883212,29932.200470611395,29360.804889570718,28614.343069115886,28027.841014436475,27712.19109699191,27584.922825665824,27742.86537969188,28162.951987945067,28867.658429767325,29569.388805824827,30005.926695549657,30151.02723935069,29746.12851686962,29202.6874502211,28874.035713548947,28824.422556412872,29047.62145824122,29276.28703406299,29299.59502632395,28926.697302790562,28397.726489034714,27929.073020322627,27529.731429760956,27118.162942870928,26917.89814054282,26896.652588994213,27042.29588035113,27214.102619165787,27363.55720447068,27384.32332901159,27321.298276716814,27279.2715243063,27199.122786623426,27080.285193998716,27081.98580300715,26979.038652713003,26785.92605706575,26713.032045046508,26788.308115783235,27163.33763110556,27682.882728974248,28071.363337889867,28113.558945095545,27841.108717835043,27516.94067902732,27313.63045985787,27224.86108182906,26871.749521971797,26660.65389063963,26638.699752004177,26692.091035182035,26760.679249924608,26613.826837623754,26404.048876746703,26201.417092022108,26080.456753401057,25847.096411220904,25617.137287057616,25673.996124064724,26040.269312152144,26434.288961401966,26774.95652590852,27309.05931933166,28392.072868660936,29646.67907304672,30738.014751566196,31409.1944707559,31600.811495360278,31355.326775834546,31055.24165218702,30717.91500041334,30509.768297468137,30433.786654908763,30554.433406053082,30727.660334305634,31004.099756101874,31234.885062655783,31242.010131958843,31039.1280803057,30779.12184839591,30615.477429768885,30523.065612587263,30540.34006118533,30685.458696571557,30829.30790572986,31123.449943640473,31290.483519227564,31247.326042742294,31056.743253758294,30767.820638176752,30462.10601626904,30263.66544878154,30156.982386144402,30155.254639758175,30185.696747114795,30275.73555137153,30160.661008467607,29921.846053350833,29733.931773183256,29623.784171981068,29593.79435425435,29635.513549716532,29659.798487578082,29660.238716151187,29672.357062968367,29709.652865850105,29710.476032976527,29660.08795294122,29582.767533077335,29496.8928086799,29415.42037001351,29551.740464466042,29794.88533897308,29977.715883700555,30028.426597005193,29993.135944816022,29875.956747500953,29747.83214114225,29627.75829019581,29431.316842872475,28630.74309109614,27555.29182437883,26671.71086445864,26178.956389001163,26000.160267772415,25990.448101786315,26186.039244605432,26418.093977387267,26523.486506947083,26524.415208320483,26452.50115716603,26343.384776320017,26565.45898460201,27066.822039348626,27203.742171376827,26907.824142852798,26512.21544936992,26199.059155418217,25978.266434420948,25836.787222923333,25762.60569309097,25865.25131696518,26015.17326822097,26109.26157229743,26103.360700259305,25875.819817983895,25726.718018590327,25835.82233837954,26164.468044523295,26542.479656138778,26836.35333553565,26956.674438146438,26998.80069427553,27105.06463518896,27235.863780892483,27203.99846883377,27058.264719550876,26895.889727151778,26702.234383948642,26495.78829001187,26357.873120797885,26344.232065560034,26547.355338349123,26866.79242762807,27154.64462441884,27515.119459450914,27837.28536283027,27969.776071749628,28009.984619847935,27980.709419736464,27968.775004035444,28054.812552699645,28134.931137740525,28079.20905533657,27921.727836733015,27599.028231590375,27235.981376196258,26988.105552160443,26921.83607558717,27013.14430607183,27492.857763867127,28178.426323816122,28751.62201758381,29112.76624126814,29499.139180508413,29967.819786598295,30339.7073966249,31244.048450557602,32769.6666011808,34373.46753722889,35414.36086095747,35645.70399138826,35417.56307153718,35087.6569849581,34826.52304423775,34707.90556589959,34885.990084777295,35210.08274198003,35375.02070894858,35520.12426801381,35603.61391842956,35633.621827741474,35702.39095833601,35903.70507260549,36444.46255411659,37235.05879665728,37925.05376445092,38265.012741870625,38172.37779513825,37543.81582014213,37282.29592560427,37209.93260008411,37196.635284964985,37287.95557650644,37546.155665160826,37891.762232425914,37832.5997335705,37757.471410779544,37883.895408129814,38183.965455456346,38513.86852677123,38619.29723950144,38455.091981733334,38363.43699586575,38460.866212675086,38591.01707657575,38891.165520771465,39505.564799823245,40197.8121699738,41200.924233284546,42715.515562730725,44190.3446031801,44940.28302325678,45203.35276359311,45099.20855341182,44768.1174680024,43734.445702031255,42622.91378390454,42180.24586206244,42426.87336671972,42773.26691794084,43005.53271921663,42911.35395721419,42763.17784392982,42834.627544397576,43270.7252055493,43945.25488326448,44536.31903267751,44797.594690815225,44601.189426662284,44275.42028256471,43772.52547360482,43523.96216933156,43462.05276479054,43279.67149442877,43077.8266936602,42997.415627991984,43367.8649569955,44330.023701950704,44690.45632328399,44886.321855145245,44954.89197830262,44951.71690510071,44880.46018154171,45343.78567841355,46212.01592829486,46923.61526407683,47539.07290086258,46844.29873931297,45427.453229417064,43884.76265289716,42840.60379804068,42579.614590001904,42766.13883337358,42726.18356746802,42501.573521994374,42391.07313488086,42334.31681685653,41811.584584729135,41053.734111393365,40641.852037026605,40532.04516067894,41011.64705369886,41906.39052168504,42721.298839465075,43398.64477394245,43889.88558677185,43998.86326546469,43860.57721875419,43757.88938118119,43700.74710933934,43530.55655213553,43370.439992621745,43360.90271195921,43715.63648395572,44565.98923239909,45961.1007179461,47482.41608655683,48660.18732861482,49601.40204844135,50196.18100334797,50908.31404089323,51799.3939628772,52475.88356232196,52576.83762298033,52425.510558607646,52200.21303289655,52023.2833602072,51878.54464811009,51729.931321516815,51467.81741993168,51360.52527392627,51503.732231810114,52311.21998439555,54259.21634406344,57827.959424579756,61287.77608589462,63375.13494159218,63873.57620532963,63473.25766916737,64172.85926870013,64518.72213342216,65073.343799721864,66084.74180905243,67342.14316335101,68269.14995827021,68789.84387179885,69533.32962648892,70331.8138001751,71418.436620751,71732.27134914756,70826.86590695249,68874.09638405703,67407.4115722101,66935.04631326861,65756.80167473132,65332.40926920146,65754.26885280387,65641.61858231622,65231.271277426924,65424.08536191844,66891.22849392369,68789.37349058375,69923.72794343533,70434.74285888406,70304.89352140333,69817.934383737,69690.17160908222,69465.06404501568,68213.17459367351,66764.58739755106,66456.11380890095,66875.92904337619,67801.75988525765,68895.29369137845,70166.09487977893,70547.04335872458,70357.39530164238,70207.78392259912,69295.34085376277,67167.63776307793,65473.04119145906,64527.56288805465,63999.9941721515,63365.08205075155,63164.68759206287,63357.580073423575,64178.76918653085,65026.13682341687,65892.68154061075,66514.83306391905,66154.34616783018,65196.2158158453,64290.84655682062,63514.06022431295,63092.77957143682,62977.144189391816,62458.319739666775,60998.962019824256,59675.568593258744,59943.583366882005,61692.792403679465,63479.90934199113,64451.52395547549,64326.824689247245,63269.54038933101,62414.06772227714,61705.92086400343,61117.55839131302,60937.492843856344,61395.535597850925,61882.83244510758,63149.21325619182,64728.0960488965,66133.98710395618,67067.98931181787,67190.24621404464,67966.24254733209,69181.94279775368,69774.71961723194,69329.28669811903,68605.06245113439,68381.53488550831,68318.37716158877,68530.21756317817,68510.48567425764,68217.3778719674,68075.28656183732,67903.40444141768,67755.2886334173,67703.58288292687,68040.52056561883,68949.03172993923,70192.40607518244,70996.18505280268,70894.19072599553,70238.14763308656,69641.72234392709,69298.86268234761,68570.85729405699,68166.41990699842,67759.03962208128,67147.61640879426,66671.8680234211,66520.42939427304,66483.32355303587,66192.35055779877,65831.16412041565,65580.41474959785,65131.68313145085,64742.70198867814,64265.626887057275,62939.495600598755,61936.69109423634,61463.92782042054,61441.427918965004,61439.92631739373,61379.994926167485,61725.59847816837,62460.9671416338,62842.904627236836,62288.34326622112,60603.16034943262,58431.60928676086,57384.73970938917,56948.06914803957,56801.34639209929,57285.48324247503,58050.2627929409,58536.275137648634,58619.37581898267,58864.4142794069,59578.838917743655,61320.63643513912,63341.30970780389,64718.10949386826,65032.39048136632,65309.64402449638,65939.6111126094,66629.52165300546,67141.23007922004,66960.74239477546,66328.71889647849,65554.90561447137,65796.56094846393,66712.76103649267,67571.06805189392,67948.76607603267,67438.20948074224,66399.16149870366,65294.53755085765,63990.76746370149,62476.16407319852,60744.316927660904,57941.31526689005,56324.41602315999,55912.01230808674,57546.020160883905,60060.60577045871,61857.938423877764,61959.02817142511,60986.60546713533,60216.75424228615,59611.87415231171,58700.72764708108,58255.67465125729,58493.33174492152,58818.24455398648,59036.9296053084,59331.870688231844,59918.33957500506,60482.392987720326,61672.300668180695,63168.66774080601,64248.61476644455,64323.13400586724,62781.477664967715,60688.07018928626,59393.394138955344,58779.60695853603,58766.50865085406,58594.415461940465,58610.11292736227,58617.078187662766,58345.8370813462,57825.842709111814,56603.81040387159,55433.366253818014,54812.68316417479,55288.78132019507,56486.474412818156,57479.67228744145,58299.62514222699,59286.24371036731,60124.354486161326,60225.95079809416,59553.86649964465,59221.25270581456,59697.38704500523,61088.81086243643,62583.66427243368,63534.44341030054,63834.49235077768,63721.33551590454,63605.37448532601,63456.106846401475,63674.91853881977,64484.432548947174,65325.172635123025,65761.02907513881,65185.66239114758,63587.97038036737,61978.82639615803,60944.705355821185,60914.130576839794,61556.75574406167,62354.697170191976,62979.85189664283,63102.138951511595,62507.257477622195,61454.57447087416,61189.26137397352,61836.795391311935,62552.914608128725,63778.75217678853,65462.6747131417,67076.92655490474,67927.67128769403,68342.91538164306,68431.0636152468,68429.89972326586,68042.9870517339,67444.4028334077,66746.19428593504,66750.60260219449,66877.92514827616,66992.33633305623,67303.10755302625,68142.02038909729,69987.44650649595,71761.47719818245,72201.20523743019,71482.07075641405,70371.87460032765,69232.2735877692,68305.37534236118,68224.97030722136,70496.0371495284,74056.46714664712,76878.00665171556,78017.59560321721,78966.48115516688,82228.29144703895,86136.99048971078,89513.35066851608,90435.37021644956,90307.90293768632,89980.57188673348,89282.94830051549,88982.17569663051,89630.48162160396,91107.21932429152,93891.74947657717,96634.30112067949,97682.04512466899,97254.17913478149,95358.09054830538,92965.5507724634,92541.01966478037,93553.4669860523,95232.1731153406,96170.95753222246,96348.62292937645,95862.63470678231,95102.70973167739,95565.96889273684,96585.34529113887,98292.99795674092,99634.15731996899,100217.95468266158,99160.67641327374,97469.92128824485,97610.22153144052,98898.74041227637,100030.5379210869,100661.54226008449,101414.64670757047,102833.96171884565,104178.49214744859,103190.57701570255,100395.69847668476,97555.20502085921,96547.40723698268,95859.31791616303,95164.20906028729,95705.62989992225,97115.04881409516,97334.18012451859,96086.87990528783,94772.11013433246,93612.63853070031,92690.02196045536,92602.45868810629,93072.01372081398,94413.06453453087,96051.63749732585,97172.7368487571,97448.00031751559,98290.57971485304,98179.7506639418,96453.8737415189,94247.09233194195,93347.39176961205,93528.4945679533,93989.45006716445,93950.99941809436,94756.1774783031,96832.07229951474,98573.93012219419,100876.96479554272,102534.46155645279,102437.7560030513,102191.44510113494,102730.74319477356,103335.38206361205,103498.16411667806,103629.20146825333,103488.50924071175,102603.61965612904,101036.03204311513,100156.71466677282,100598.113162387,102017.46435683256,102544.71345473058,101564.92747526598,99409.94830463386,98068.16779698072,97925.85938782828,97750.0212407791,97493.88660810127,97328.75264895977,96810.77850373894,96204.84910182313],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"BTC-USD Price Prediction\"},\"xaxis\":{\"title\":{\"text\":\"Time\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('26412f81-f463-4203-ba9b-edf54c270cd8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-0cf8482df36d>:203: DeprecationWarning:\n",
            "\n",
            "Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted prices for next 30 days:\n",
            "Day 1: $96,204.85\n",
            "Day 2: $95,676.09\n",
            "Day 3: $95,270.55\n",
            "Day 4: $94,993.96\n",
            "Day 5: $94,825.28\n",
            "Day 6: $94,734.35\n",
            "Day 7: $94,693.29\n",
            "Day 8: $94,681.18\n",
            "Day 9: $94,684.70\n",
            "Day 10: $94,696.33\n",
            "Day 11: $94,712.22\n",
            "Day 12: $94,730.42\n",
            "Day 13: $94,749.79\n",
            "Day 14: $94,769.52\n",
            "Day 15: $94,788.88\n",
            "Day 16: $94,807.28\n",
            "Day 17: $94,824.27\n",
            "Day 18: $94,839.65\n",
            "Day 19: $94,853.26\n",
            "Day 20: $94,865.16\n",
            "Day 21: $94,875.39\n",
            "Day 22: $94,884.15\n",
            "Day 23: $94,891.55\n",
            "Day 24: $94,897.80\n",
            "Day 25: $94,902.99\n",
            "Day 26: $94,907.31\n",
            "Day 27: $94,910.87\n",
            "Day 28: $94,913.80\n",
            "Day 29: $94,916.19\n",
            "Day 30: $94,918.12\n"
          ]
        }
      ]
    }
  ]
}